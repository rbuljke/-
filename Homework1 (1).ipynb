{"cells":[{"cell_type":"markdown","metadata":{"id":"JQjq1AU49Q-q"},"source":["# Домашнее задание №1: линейная регрессия (максимум 10 баллов)\n","\n","Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"1FsuljEb9Q-w","executionInfo":{"status":"ok","timestamp":1727453693046,"user_tz":-180,"elapsed":444,"user":{"displayName":"Обыкновенный","userId":"00368550710020582022"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"ChBks_AF9Q-y"},"source":["## Многомерная линейная регрессия из sklearn"]},{"cell_type":"markdown","metadata":{"id":"jihDOyOd9Q-0"},"source":["Создадим набор данных для многомерной регрессии"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ykkouQff9Q-2","outputId":"b81e1166-1a54-4f8a-8067-06274e645f85","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727453701347,"user_tz":-180,"elapsed":6646,"user":{"displayName":"Обыкновенный","userId":"00368550710020582022"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 100) (10000,)\n"]}],"source":["from sklearn.datasets import make_regression\n","\n","X, y = make_regression(n_samples = 10000)\n","print(X.shape, y.shape)"]},{"cell_type":"markdown","metadata":{"id":"PMZP4pwT9Q-3"},"source":["У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"a-cl2ujU9Q-4","outputId":"0d714760-d660-42c4-8410-5a2541933262","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727453707299,"user_tz":-180,"elapsed":1852,"user":{"displayName":"Обыкновенный","userId":"00368550710020582022"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["7.428539052184333e-26\n"]}],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","reg = LinearRegression().fit(X, y)\n","print(mean_squared_error(y, reg.predict(X)))"]},{"cell_type":"markdown","metadata":{"id":"8IKaW67f9Q-6"},"source":["Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Fzw6etrW9Q-7","outputId":"ae40440c-fc81-4f47-cb81-04c624624d79","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727453720098,"user_tz":-180,"elapsed":294,"user":{"displayName":"Обыкновенный","userId":"00368550710020582022"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1.7487924601078848e-12\n"]},{"output_type":"execute_result","data":{"text/plain":["array([-1.36192992e-08, -3.18970795e-08, -4.25487890e-09,  1.79027325e-08,\n","        1.98973870e-08, -3.70688759e-09,  8.45375504e-09,  1.94269364e-09,\n","        7.44169279e-10,  1.30634957e-08,  2.33698086e+01,  1.17380405e-09,\n","        2.12368048e+01,  8.91195913e-09, -2.60043794e-08,  4.30882260e-09,\n","        2.00497097e-08,  2.56210707e-08,  7.16311479e-09, -5.30880921e-09,\n","       -1.49042592e-08,  1.71050572e-08, -7.48671701e-12,  4.25570830e+01,\n","        2.29791635e+01,  1.87074108e-08, -1.92207031e-08,  4.51697611e-08,\n","        2.11286671e-08,  1.81946304e-09, -2.52408996e-09,  3.23697705e+01,\n","       -2.54303161e-08, -7.50465996e-09, -1.15885678e-08, -9.14593645e-09,\n","        3.05058084e-08,  1.92745482e-08, -1.24214421e-08,  9.14923405e+00,\n","       -5.45233529e-08, -4.49926713e-08, -3.02530642e-08,  9.15664239e-09,\n","        3.50084346e-08, -1.37939808e-08, -4.87595525e-09, -3.11572059e-08,\n","        8.86164126e-09, -1.52164846e-08, -5.72453937e-09, -3.23876085e-08,\n","       -5.71118037e-09, -5.96243937e-09, -2.64139276e-08, -2.02984423e-08,\n","       -2.89548688e-08,  3.47445715e-09, -2.30000459e-08,  9.70151461e+01,\n","       -3.09996153e-09, -3.17389387e-08, -1.32975061e-08,  2.72933457e-09,\n","       -1.81953788e-08, -1.87641265e-08,  3.22754261e-08,  5.17175223e-08,\n","       -2.00879338e-08,  2.16232948e-08, -1.26306794e-08,  1.32744046e-08,\n","        3.13024388e-08,  1.78741347e-08,  1.22122850e-08, -1.38463759e-08,\n","        3.08684378e-08,  9.25473308e-09, -7.73719930e-09,  1.11789905e-08,\n","       -9.90517436e-09,  1.47094741e-08,  1.32288517e-08, -9.25854608e-09,\n","        1.01286964e-08, -9.86815028e-09,  3.98522875e+01,  1.92689979e-08,\n","        2.77530383e+00,  3.26326869e-08,  3.18169144e+01,  3.81867443e-08,\n","        3.89451659e-08,  2.48090077e-08, -7.29373499e-09, -2.70593215e-08,\n","        6.22622697e-08,  3.31703180e-09, -1.32812015e-08,  7.43761416e-09])"]},"metadata":{},"execution_count":4}],"source":["from sklearn.linear_model import SGDRegressor\n","reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n","print(mean_squared_error(y, reg.predict(X)))\n","reg.coef_"]},{"cell_type":"markdown","metadata":{"id":"oq1076z-9Q-8"},"source":["***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n","\n","***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."]},{"cell_type":"markdown","metadata":{"id":"3jJplHqS9Q-9"},"source":["## Ваша многомерная линейная регрессия"]},{"cell_type":"markdown","metadata":{"id":"SKeXXhEH9Q--"},"source":["***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс.\n","\n","Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n","\n","***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"VY3CTs0W9Q-_","executionInfo":{"status":"ok","timestamp":1727454005230,"user_tz":-180,"elapsed":282,"user":{"displayName":"Обыкновенный","userId":"00368550710020582022"}}},"outputs":[],"source":["class LinearRegression(object):\n","    def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000):\n","        '''\n","        Для начала необходимо инициализировать параметры\n","        alpha - это learning rate или шаг обучения\n","        l_ratio - параметр регуляризации\n","        tol - значение для критерия останова\n","        max_iter - максимальное количество итераций обучения\n","        '''\n","\n","        self.aplha = alpha\n","        self.l_ratio = l_ratio\n","        self.tol = tol\n","        self.max_iter = max_iter\n","        self.bias = None\n","        self.weights = None\n","\n","    def fit(self, X, y):\n","        '''\n","        Метод для обучения линейной регрессии\n","        X - матрица признаков\n","        y - вектор правильных ответов\n","        '''\n","\n","        n_samples, n_features = X.shape\n","        self.bias, self.weights = 0, np.zeros(n_features)\n","        previous_db, previous_dw = 0, np.zeros(n_features)\n","\n","        while True:\n","            y_pred = X @ self.weights + self.bias\n","            db = 1 / n_samples * np.sum(y_pred - y)\n","            dw = 1 / n_samples * X.T @ (y_pred - y)\n","            self.bias -= self.l_ratio * db\n","            self.weights -= self.l_ratio * dw\n","\n","            abs_db_reduction = np.abs(db - previous_db)\n","            abs_dw_reduction = np.abs(dw - previous_dw)\n","\n","            if abs_db_reduction < self.tol:\n","                if abs_dw_reduction.all() < self.tol:\n","                    break\n","\n","            previous_db = db\n","            previous_dw = dw\n","\n","\n","\n","    def predict(self, X):\n","        '''\n","        Метод для предсказаний линейной регрессии\n","        X - матрица признаков\n","        '''\n","\n","        return X @ self.weights + self.bias"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"vH6XOYJm9Q-_","colab":{"base_uri":"https://localhost:8080/","height":347},"executionInfo":{"status":"error","timestamp":1727454312274,"user_tz":-180,"elapsed":170039,"user":{"displayName":"Обыкновенный","userId":"00368550710020582022"}},"outputId":"6716e66b-43f2-4ad7-bc10-edbbea9e5e56"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-20d64d0a03d0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You are amazing! Great work!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-adca67052dae>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_ratio\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m def _sum_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n\u001b[0m\u001b[1;32m   2173\u001b[0m                     initial=None, where=None):\n\u001b[1;32m   2174\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["my_reg = LinearRegression()\n","my_reg.fit(X, y)\n","assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n","print('You are amazing! Great work!')"]},{"cell_type":"markdown","metadata":{"id":"uBRR_3Sh9Q_A"},"source":["***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrSPmUZl9Q_B"},"outputs":[],"source":["#your code here"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
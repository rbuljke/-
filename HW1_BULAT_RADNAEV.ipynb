{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQjq1AU49Q-q"
      },
      "source": [
        "# Домашнее задание №1: линейная регрессия (максимум 10 баллов)\n",
        "\n",
        "Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1FsuljEb9Q-w"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "variant = len('Radnaev') % 4 + 1\n",
        "print(variant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChBks_AF9Q-y"
      },
      "source": [
        "## Многомерная линейная регрессия из sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jihDOyOd9Q-0"
      },
      "source": [
        "Создадим набор данных для многомерной регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ykkouQff9Q-2",
        "outputId": "8223f891-c775-432e-c125-53bff94c60c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 100) (10000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y = make_regression(n_samples = 10000)\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMZP4pwT9Q-3"
      },
      "source": [
        "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a-cl2ujU9Q-4",
        "outputId": "dc936815-cd0a-4c42-bc79-824337410051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.6307991658961891e-25\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "reg = LinearRegression().fit(X, y)\n",
        "print(mean_squared_error(y, reg.predict(X)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IKaW67f9Q-6"
      },
      "source": [
        "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fzw6etrW9Q-7",
        "outputId": "9a1effb5-b3d9-47cc-bd8b-c696505c8521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7770664529808338e-12\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([-1.08714506e-09,  7.92405620e+01,  2.61857128e-08,  1.34422827e-08,\n",
              "        4.10305900e-08, -3.11657198e-08,  6.15275078e-09, -1.68028578e-08,\n",
              "       -1.49174276e-09,  3.04118450e-08,  6.26882182e-08,  1.09625896e-08,\n",
              "        7.58596228e-08,  1.49992283e-08, -4.06921612e-09,  2.25734012e-08,\n",
              "        1.93077055e-08,  4.55846649e-08,  2.37434067e-08,  9.85693079e-09,\n",
              "        6.50897283e-08,  4.45478115e-08,  9.41942395e+01, -1.52533051e-08,\n",
              "        3.41428433e-08,  7.15022805e-09, -3.79269384e-08, -9.83465591e-09,\n",
              "       -3.87311643e-08, -7.40013651e-08, -3.87748354e-08,  5.93993545e-10,\n",
              "       -1.37272381e-08,  2.63130888e-09, -3.14397194e-08, -6.04534018e-09,\n",
              "        2.04614574e+01, -2.65953054e-08, -1.58126790e-08,  5.16605172e+01,\n",
              "        5.28073427e-08,  2.05023140e-08,  7.99913803e-10, -1.56483942e-08,\n",
              "       -2.41408432e-08, -5.23435808e-08, -4.16183908e-08, -3.25128623e-08,\n",
              "        7.71380708e-08,  4.43503838e+01,  2.66237968e+01, -6.05424583e-08,\n",
              "       -6.84239871e-08, -2.11310860e-08,  3.47252124e-08,  9.08402997e+00,\n",
              "        4.39031164e-08,  9.58542477e-09, -1.18938508e-08,  9.08995950e-09,\n",
              "       -3.15235361e-08, -1.13406381e-09, -3.06329380e-09, -2.96168188e-08,\n",
              "        2.32826565e+01, -3.70333788e-08, -2.36805291e-09, -5.22019893e-08,\n",
              "       -1.10640480e-08,  2.70108738e-08, -9.53845511e-09,  7.41440594e-09,\n",
              "       -5.71950367e-09, -3.26150841e-09,  5.45941581e-09,  2.36999408e-08,\n",
              "       -1.26286770e-08, -1.09175979e-08,  2.44822341e-09, -3.27080407e-08,\n",
              "       -6.75005370e-08,  1.37230616e-08,  1.00618195e-09, -4.35980337e-08,\n",
              "       -3.46625267e-08, -1.30881604e-08,  2.62309612e-08, -5.43789352e-08,\n",
              "       -2.00394734e-09,  3.03127741e-08,  6.03524045e-09, -2.07016447e-09,\n",
              "        1.75074318e-08,  7.28835301e+01,  1.24536835e-08,  1.82971610e-08,\n",
              "       -4.03949202e-08,  3.35277190e+01,  1.71310994e-08,  5.50681100e-08])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
        "print(mean_squared_error(y, reg.predict(X)))\n",
        "reg.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq1076z-9Q-8"
      },
      "source": [
        "***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n",
        "1) Разница вызвана тем, что LinearRegression решает задачу аналитическим методом, а SGDRegressor является численным методом, в этом случае результат зависит от значений гиперпараметров.\n",
        "\n",
        "***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE для SGDRegressor: 2.3186866294852795e-27\n"
          ]
        }
      ],
      "source": [
        "reg = SGDRegressor(alpha=1e-6, max_iter=100000, tol=1e-4, penalty=None, learning_rate='constant', eta0=0.01)\n",
        "#Уменьшили шаг, увеличили кол-во итераций, усилили критерий остановки.\n",
        "\n",
        "reg.fit(X, y)\n",
        "\n",
        "# Рассчитываем MSE\n",
        "print(\"MSE для SGDRegressor:\", mean_squared_error(y, reg.predict(X)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jJplHqS9Q-9"
      },
      "source": [
        "## Ваша многомерная линейная регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKeXXhEH9Q--"
      },
      "source": [
        "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс.\n",
        "\n",
        "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
        "\n",
        "***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VY3CTs0W9Q-_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LinearRegression(object):\n",
        "    def __init__(self, alpha=0.1, l_ratio=0.00001, tol=1e-6, max_iter=10000):\n",
        "        self.alpha = alpha\n",
        "        self.l_ratio = l_ratio\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.w = None  # Weights will be initialized during fit\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "\n",
        "        mse_prev = float('inf')\n",
        "\n",
        "        for iteration in range(self.max_iter):\n",
        "            \n",
        "            y_pred = X @ self.w\n",
        "            \n",
        "            error = y_pred - y\n",
        "            \n",
        "            mse = (1 / n_samples) * np.dot(error, error)\n",
        "\n",
        "            \n",
        "            if abs(mse_prev - mse) < self.tol:\n",
        "                \n",
        "                break\n",
        "            mse_prev = mse\n",
        "\n",
        "            \n",
        "            reg = self.l_ratio * np.sign(self.w)\n",
        "            reg[0] = 0 # убираем регуляризацию у свободных весов\n",
        "            grad = (2 / n_samples) * (X.T @ error) + reg\n",
        "\n",
        "            \n",
        "            self.w -= self.alpha * grad\n",
        "\n",
        "        else:\n",
        "            print(f\"Достигли макс. итераций: {self.max_iter}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X @ self.w\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vH6XOYJm9Q-_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 1.6824027281759088e-06\n",
            "You are amazing! Great work!\n"
          ]
        }
      ],
      "source": [
        "my_reg = LinearRegression()\n",
        "my_reg.fit(X, y)\n",
        "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
        "mse = mean_squared_error(y, my_reg.predict(X))\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print('You are amazing! Great work!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBRR_3Sh9Q_A"
      },
      "source": [
        "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LrSPmUZl9Q_B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09998481668184668\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "X, y = make_regression(n_samples=10000, n_features=100)\n",
        "model = Lasso(alpha = 0.1)\n",
        "model.fit(X, y)\n",
        "mse = mean_squared_error(y, model.predict(X))\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Встроенные модели scikit-learn обычно более оптимизированы и эффективны для реальных задач. Они реализуют продвинутые методы оптимизации и обеспечивают лучшую производительность."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
